{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "## cleaning function\n",
    "def utils_preprocess_text(txt, punkt=True, lower=True, slang=True, stopwords=None, stemm=False, lemm=True):\n",
    "    ### separate sentences with '. '\n",
    "    txt = re.sub(r'\\.(?=[^ \\W\\d])', '. ', str(txt))\n",
    "    ### remove punctuations and characters\n",
    "    txt = re.sub(r'[^A-Za-z0-9\\s]+', '', txt) if punkt is True else txt\n",
    "    ### remove extra spaces\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    ### remove numeric characters\n",
    "    txt = re.sub(r'\\d+', '', txt)\n",
    "    ### remove urls\n",
    "    txt = re.sub(r'http\\S+', '', txt)\n",
    "    ### remove email addresses\n",
    "    txt = re.sub(r'\\S+@\\S+', '', txt)\n",
    "    ### strip\n",
    "    txt = \" \".join([word.strip() for word in txt.split()])\n",
    "    ### lowercase\n",
    "    txt = txt.lower() if lower is True else txt\n",
    "    ### slang\n",
    "    txt = contractions.fix(txt) if slang is True else txt   \n",
    "    ### tokenize (convert from string to list)\n",
    "    lst_txt = txt.split()\n",
    "    ### stemming (remove -ing, -ly, ...)\n",
    "    if stemm is True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_txt = [ps.stem(word) for word in lst_txt]\n",
    "    ### lemmatization (convert the word into root word)\n",
    "    if lemm is True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_txt = [lem.lemmatize(word) for word in lst_txt]\n",
    "    ### remove Stopwords\n",
    "        ### remove stopwords\n",
    "    if stopwords is not None:\n",
    "        lst_txt = [word for word in lst_txt if word not in stopwords]\n",
    "    ### back to string\n",
    "    txt = \" \".join(lst_txt)\n",
    "    return txt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtf_train = pd.read_csv(\"cnn_dailymail/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to both text and summaries\n",
    "# create stop words\n",
    "stopwords = []\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stopwords += [\"cnn\", \"say\", \"said\", \"new\", \"one\", \"two\", \"also\"]\n",
    "stopwords += [\"just\", \"like\", \"get\", \"make\", \"time\", \"even\", \"much\", \"many\", \"way\", \"thing\", \"need\", \"take\", \"well\", \"could\", \"would\", \"should\", \"might\", \"must\", \"also\", \"however\", \"yet\", \"still\", \"rather\", \"either\", \"neither\", \"whether\", \"meanwhile\"]\n",
    "\n",
    "dtf_train[\"text_clean\"] = dtf_train[\"article\"].apply(utils_preprocess_text, stopwords=stopwords, stemm=False, slang=False, lemm=False)\n",
    "dtf_train[\"y_clean\"] = dtf_train[\"highlights\"].apply(utils_preprocess_text, stopwords=stopwords, stemm=False, slang=False, lemm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'associated press published est october updated est october bishop fargo catholic diocese north dakota exposed potentially hundreds church members fargo grand forks jamestown hepatitis virus late september early october state health department issued advisory exposure anyone attended five churches took communion bishop john folda pictured fargo catholic diocese north dakota exposed potentially hundreds church members fargo grand forks jamestown hepatitis state immunization program manager molly howell says risk low officials feel important alert people possible exposure diocese announced monday bishop john folda taking diagnosed hepatitis diocese says contracted infection contaminated food attending conference newly ordained bishops italy last month symptoms hepatitis include fever tiredness loss appetite nausea abdominal discomfort fargo catholic diocese north dakota pictured bishop located'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf_train[\"text_clean\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf_train[\"article\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding is done and a simple encoder decoder model is given to see the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "max_features = 10000\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(dtf_train[\"text_clean\"].values)\n",
    "\n",
    "# Create padded sequences\n",
    "maxlen = 500\n",
    "X = pad_sequences(tokenizer.texts_to_sequences(dtf_train[\"text_clean\"].values), maxlen=maxlen)\n",
    "y = pad_sequences(tokenizer.texts_to_sequences(dtf_train[\"y_clean\"].values), maxlen=maxlen)\n",
    "\n",
    "# Define the encoder-decoder model\n",
    "latent_dim = 256\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, latent_dim)(encoder_inputs)\n",
    "x, state_h, state_c = LSTM(latent_dim, return_state=True)(x)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, latent_dim)(decoder_inputs)\n",
    "x = LSTM(latent_dim, return_sequences=True)(x, initial_state=encoder_states)\n",
    "decoder_outputs = Dense(max_features, activation=\"softmax\")(x)\n",
    "\n",
    "# Define the model and compile it\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit([X, y], y, batch_size=64, epochs=10, validation_split=0.2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
